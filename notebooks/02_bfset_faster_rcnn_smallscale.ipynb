{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hamda-Bahri/bfset-experiments/blob/main/notebooks/02_bfset_faster_rcnn_smallscale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PmiEvxeQGep"
      },
      "source": [
        "# BFSET – Faster R-CNN Small-Scale Experiment (500 Images)\n",
        "\n",
        "This notebook trains and evaluates a Faster R-CNN detector on a small-scale subset\n",
        "(≈500 images) of the BFSET dataset. The goal is to obtain a clean, reproducible\n",
        "baseline that can be referenced in the experimental section of the associated article.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hslVExswPv5x"
      },
      "outputs": [],
      "source": [
        "import sys, platform\n",
        "import torch, torchvision\n",
        "\n",
        "print(f\"Python version : {sys.version.split()[0]}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision    : {torchvision.__version__}\")\n",
        "print(f\"CUDA available : {torch.cuda.is_available()}\")\n",
        "print(f\"Platform       : {platform.platform()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X61xKCVmQozx"
      },
      "source": [
        "## 2. Dataset location (Google Drive)\n",
        "\n",
        "Mount Google Drive and define the paths to the BFSET small-scale subset\n",
        "(images and YOLO-format labels)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "As5KFXqvQsl5"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5890xR5Q8u_"
      },
      "outputs": [],
      "source": [
        "DATA_ROOT = \"/content/drive/MyDrive/BFSET_TEST\"\n",
        "\n",
        "TRAIN_IMG   = f\"{DATA_ROOT}/images/train\"\n",
        "TRAIN_LABEL = f\"{DATA_ROOT}/labels/train\"\n",
        "\n",
        "VAL_IMG     = f\"{DATA_ROOT}/images/val\"\n",
        "VAL_LABEL   = f\"{DATA_ROOT}/labels/val\"\n",
        "\n",
        "print(\"Train images :\", TRAIN_IMG)\n",
        "print(\"Train labels :\", TRAIN_LABEL)\n",
        "print(\"Val   images :\", VAL_IMG)\n",
        "print(\"Val   labels :\", VAL_LABEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNp-8xheRDJY"
      },
      "source": [
        "## 3. Utilities – YOLO to COCO conversion\n",
        "\n",
        "BFSET annotations are assumed to be in YOLO format\n",
        "`class x_center y_center width height` (normalized).\n",
        "Faster R-CNN expects bounding boxes in absolute pixel coordinates\n",
        "`[x_min, y_min, x_max, y_max]`. The helper below performs this conversion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHeCpSVVRIFk"
      },
      "outputs": [],
      "source": [
        "import os, glob\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "def yolo_to_coco(box, W, H):\n",
        "    \"\"\"Convert YOLO-normalized box to COCO-style absolute coordinates.\n",
        "\n",
        "    Args:\n",
        "        box: (x_center, y_center, width, height) normalized in [0,1].\n",
        "        W, H: image width and height in pixels.\n",
        "    Returns:\n",
        "        [x_min, y_min, x_max, y_max]\n",
        "    \"\"\"\n",
        "    x_c, y_c, w, h = box\n",
        "    x_min = (x_c - w / 2.0) * W\n",
        "    y_min = (y_c - h / 2.0) * H\n",
        "    x_max = (x_c + w / 2.0) * W\n",
        "    y_max = (y_c + h / 2.0) * H\n",
        "    return [x_min, y_min, x_max, y_max]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jv5JNRp5ROsc"
      },
      "source": [
        "## 4. PyTorch Dataset for BFSET (Faster R-CNN)\n",
        "\n",
        "The dataset class reads RGB images and their corresponding YOLO label files,\n",
        "converts bounding boxes to COCO format, and returns tensors suitable for\n",
        "Faster R-CNN training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mPKx6RS8RTty"
      },
      "outputs": [],
      "source": [
        "import torchvision.transforms as T\n",
        "\n",
        "class BFSETFasterRCNNDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_dir, label_dir):\n",
        "        self.img_paths = sorted(glob.glob(os.path.join(img_dir, \"*.jpg\")))\n",
        "        if len(self.img_paths) == 0:\n",
        "            raise RuntimeError(f\"No .jpg images found in {img_dir}\")\n",
        "        self.label_dir = label_dir\n",
        "        self.transforms = T.ToTensor()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        W, H = image.size\n",
        "\n",
        "        label_path = os.path.join(\n",
        "            self.label_dir,\n",
        "            os.path.basename(img_path).replace(\".jpg\", \".txt\")\n",
        "        )\n",
        "\n",
        "        boxes = []\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) != 5:\n",
        "                        continue\n",
        "                    _, xc, yc, w, h = map(float, parts)\n",
        "                    boxes.append(yolo_to_coco([xc, yc, w, h], W, H))\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # single class: beard\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    images, targets = list(zip(*batch))\n",
        "    return list(images), list(targets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zhwtCGi_RXe8"
      },
      "source": [
        "## 5. Data loaders\n",
        "\n",
        "We now create training and validation loaders with a small batch size,\n",
        "suitable for Colab GPUs (or CPU if necessary)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEwAAwNQRbyO"
      },
      "outputs": [],
      "source": [
        "train_dataset = BFSETFasterRCNNDataset(TRAIN_IMG, TRAIN_LABEL)\n",
        "val_dataset   = BFSETFasterRCNNDataset(VAL_IMG, VAL_LABEL)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=4,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "len(train_dataset), len(val_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6sGIJkqRgWI"
      },
      "source": [
        "## 6. Faster R-CNN model\n",
        "\n",
        "We use a pre-trained Faster R-CNN with a ResNet-50 backbone and an FPN.\n",
        "The classification head is replaced to predict two classes: background and beard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1kL7MUfURjiz"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# Load a model pre-trained on COCO\n",
        "model = fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# Replace the classifier with a new one for 2 classes (background + beard)\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1wZzoNeIRnQi"
      },
      "source": [
        "## 7. Training loop\n",
        "\n",
        "We train for a small number of epochs (e.g. 5) to obtain a first baseline on\n",
        "the 500-image subset. The goal is not to fully optimize the model, but to\n",
        "obtain meaningful detection metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lPUf4plMRorc"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4)\n",
        "\n",
        "num_epochs = 5\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for images, targets in train_loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        losses.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += losses.item()\n",
        "      print(f\"Epoch {epoch+1}/{num_epochs} – Training loss: {total_loss:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyUjfUHYRymE"
      },
      "source": [
        "## 8. Simple evaluation (mAP@IoU=0.5 approximation)\n",
        "\n",
        "This section computes a simple precision-like metric based on the IoU between\n",
        "predicted and ground-truth boxes. The result is an approximation of mAP@0.5 on\n",
        "the validation set and is sufficient for a small-scale comparison with YOLO."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZjuWNuuR0K2"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import box_iou\n",
        "import numpy as np\n",
        "\n",
        "def evaluate_map50(model, data_loader, score_threshold=0.5, iou_threshold=0.5):\n",
        "    model.eval()\n",
        "    all_precisions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in data_loader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            outputs = model(images)\n",
        "\n",
        "            for output, target in zip(outputs, targets):\n",
        "                # Filter predictions by score\n",
        "                scores = output[\"scores\"].cpu()\n",
        "                keep = scores >= score_threshold\n",
        "                pred_boxes = output[\"boxes\"][keep].cpu()\n",
        "\n",
        "                gt_boxes = target[\"boxes\"].cpu()\n",
        "\n",
        "                if len(pred_boxes) == 0 or len(gt_boxes) == 0:\n",
        "                    continue\n",
        "\n",
        "                ious = box_iou(pred_boxes, gt_boxes)\n",
        "                max_iou, _ = ious.max(dim=1)\n",
        "\n",
        "                precision = (max_iou >= iou_threshold).float().mean().item()\n",
        "                all_precisions.append(precision)\n",
        "\n",
        "    if len(all_precisions) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return float(np.mean(all_precisions))\n",
        "\n",
        "map50 = evaluate_map50(model, val_loader)\n",
        "print(f\"Approximate mAP@0.5 on validation set: {map50:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMJ7o4arR5zz"
      },
      "source": [
        "## 9. Visualizing predictions\n",
        "\n",
        "To better understand the qualitative behaviour of the detector, we show a few\n",
        "validation images with predicted beard bounding boxes overlaid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AyQ9tGVR_Bp"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def show_prediction(image_path, model, score_threshold=0.5):\n",
        "    model.eval()\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    t = T.ToTensor()(img).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model([t])[0]\n",
        "\n",
        "    boxes = output[\"boxes\"].cpu().numpy()\n",
        "    scores = output[\"scores\"].cpu().numpy()\n",
        "\n",
        "    img_np = np.array(img)\n",
        "\n",
        "    for box, score in zip(boxes, scores):\n",
        "        if score < score_threshold:\n",
        "            continue\n",
        "        x1, y1, x2, y2 = box.astype(int)\n",
        "        cv2.rectangle(img_np, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(img_np)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage on one validation image (change index if needed)\n",
        "example_idx = 0\n",
        "example_path = val_dataset.img_paths[example_idx]\n",
        "print(\"Example image:\", example_path)\n",
        "show_prediction(example_path, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukcDydRjSDcL"
      },
      "source": [
        "## 10. Saving the trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z7mSCTFJSKX6"
      },
      "outputs": [],
      "source": [
        "MODEL_PATH = \"faster_rcnn_bfset_smallscale.pth\"\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(\"Model saved to\", MODEL_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7s04quhSQaX"
      },
      "source": [
        "## 11. Conclusion\n",
        "\n",
        "This notebook provides a complete, reproducible Faster R-CNN baseline on a\n",
        "500-image subset of the BFSET dataset. The results (training loss, approximate\n",
        "mAP@0.5 and qualitative visualizations) can be directly used in the experimental\n",
        "section of the BFSET article and the notebook can be published on GitHub as:\n",
        "\n",
        "`notebooks/02_bfset_faster_rcnn_smallscale.ipynb`."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNe6yhjyF1+IyErTBIw0eHm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}